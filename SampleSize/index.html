<!DOCTYPE html>
<html>
  <head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="css/bootstrap.min.css" rel="stylesheet">
		<link href="css/styles.css" rel="stylesheet">
		<link href="css/vis.css" rel="stylesheet">
    <style>
    .graph-container {
        float: right;
        width: 41%;
        padding: 1px;
    }

    .clearfix::after {
        content: "";
        clear: both;
        display: table;
    }

    .img-responsive {
      display: inline-block;
      height: inherit;
      width: 45%
    }

    .page_full_heading {
      width: 100%;
      height: auto;
      float: left;
      margin: 0px;
      color: #000000;
      font-family: helvetica;
      /*font: 30px/46px 'Fira Sans';*/
      font-weight: bold;
    }

    .explain {
      color: #30394F;
      font-family: helvetica;
      text-align:left;
    }

    body {
      margin:100px 0;
      padding:50px;
      font-family: helvetica;
      /*text-align:center;*/
    }

    h3 {
      font-family: helvetica;
      /*text-align:center;*/
    }
    .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 50%;
    }
    </style>
  </head>
  <body>
    <div class="hero-image"><center><img class="img-responsive" src="icons/ab-testing-3.png" alt="Image" style="width:80%"></center></div>
        <h3 class="explain">A/B Testing</h3>
        <p class="explain">
          A/B testing can be used to measure the true impact of user experience/interface changes in digital environments. With any business there are many factors that can contribute to performance, making it difficult to isolate the impact of one change. There are also confounding factors that can effect measurement, including events like seasonal changes, natural disasters, or promotional offers. A/B testing tools are used to serve randomized sets of customers different versions of content, allowing the business to measure the impact that the version differences can have.
      </p>
      <h3 class="explain">Experimental Design and Statistics</h3>
      <p class="explain">

        Having a true experimental design structure for A/B tests is key to ensuring proper measurement. It's also very important to understand the statistics and assumptions associated with different methodologies. Some key components to consider are:

        <ul style="list-style-type:disc" class="explain">
          <li>Metric type: is it discrete or continuous?</li>
          <li>Hypothesis test to run: what are the assumptions of the test?</li>
          <li>Sample Size: how many users need to be exposed to the content to reach statistical significance?</li>
          <li>Distribution: does the actual distribution of the test results fit with the assumed distribution of the test statistic?</li>
          <li>Outliers: are there users who are skewing the results of the data?</li>
        </ul>

        </p>

      <h3 class="explain">Example</h3> 
      <p class="explain">
        Using an example, imagine you have two variations of a website, Variation A and Variation B. Variation A is what customers are accustomed to seeing when the visit the site and Variation B has changes in the checkout process. We will use an A/B testing tool to randomly serve Variation A to half the population, and Variation B to the other half. We will then use the metric 'Average Revenue Per Visitor' to measure the performance, and a T-Test to test the null hypothesis that there is no difference in Average Revenue Per Visitor between the variations.
      </p>
      </div>
    <!-- add content -->
    <div class="clearfix">
         <div class="graph-container">
            <div class="row">
             <div class="col-md-12">
               <div class="row">
                 <div class="col-md-12" id="test"></div>
               </div>
               <div id="n_container">
               <h4 id="n">Slide me</h4>
               <div id="slider_n" class="ui-slider"></div>
             </div>
                 <div id="viz2" style="width:100%"></div>
             </div>
             </div>
         </div>
         <div class="graph-container">
           <div class="row">
           <div class="col-md-12">
             <div class="div_settings">
               <h4>Slide me <a class="glyphicon glyphicon-pencil" data-toggle="popover" title="Settings" id="slider-settings" href="javascript:void(0);"><span></span></a>
               </h4>
             </div>
             <form id="popover-content" class="hide" role='form'>
               <div class='form-group'>
                 <label for='inputCohend'>Max Difference In Means </label>
                   <input type='number' class='form-control inputCohend' id='inputCohend' min="0" max="5" placeholder='maxCohend'>
               </div>
               <div class='form-group'>
                 <label for='inputStep'>Slider step size </label>
                   <input type='number' class='form-control inputStep' id='inputStep' min="0" max="1" step="0.1" placeholder='inputStep'>
               </div>
             </form>
             <div class="row">
               <div class="col-md-12" id="test"></div>
             </div>
               <div id="slider" class="ui-slider"></div>
               <div id="viz" style="width:100%"></div>
           </div>
           </div>
         </div>
           <div id="viz3" class="center"></div>
    </div>

   <!-- add content  -->
   <!-- Load JS files =============================-->
    <script src="//code.jquery.com/jquery-1.8.3.js"></script>
		<script src="js/jquery-ui-1.10.3.custom.min.js"></script>
    <script src="js/jquery.ui.touch-punch.min.js"></script>
		<script src="js/bootstrap.min.js"></script>
	  <script src="js/jstat.js"></script>
		<!-- <script src="http://d3js.org/d3.v3.min.js"></script>-->
    <script src="js/d3.v3.min.js"></script>
    <script src="js/vis2.js"></script>
	
	 <p>
	 

<h3 class="explain">What is Difference in Means?</h3> 
In A/B testing, difference in means is the observed difference in performance between A and B. In marketing this is sometimes called “lift” and in academia this is sometimes called “effect size”. The term effect size can refer to a standardized measure of effect (such as r, Cohen's d, or the odds ratio), or to an unstandardized measure (e.g., the difference between group means or the unstandardized regression coefficients). Standardized effect size measures are typically used when:

        <ul style="list-style-type:disc" class="explain">
          <li>the metrics of variables being studied do not have intrinsic meaning (e.g., a score on a personality test on an arbitrary scale),</li>
          <li>results from multiple studies are being combined,</li>
          <li>some or all of the studies use different scales, or</li>
          <li>it is desired to convey the size of an effect relative to the variability in the population.</li>
        </ul>



<h3 class="explain">What is Sample Size?</h3> 
Sample size is the number of visitors in the A/B test. Intuitively, we sense that the larger the sample size the less likely A will outperform B (or vice versa) just by chance. People use sample size to judge whether they should believe A is actually better than B or dismiss the results as due chance. So, everyone likes to know the sample size.


<h3 class="explain">What are the A/B Testing Outcomes?</h3> 

<h4 class="explain">Positive Effect</h4> 
As an example, let’s look at the following A/B results; Group A (test): 22.6 Revenue Per Customer and Group B (control): 21.5 Revenue Per Customer. The data suggests that over many visits, Group A will have 22.6/21.5 – 1 = 5.1% more Revenue Per Customer. The difference in means would equal 1.1. In this example, let’s say the sample size would be 40. It’s relevant for the confidence you should have regarding chance effects. Variances should be equal as part of the T-Test assumptions. While Group A outperformed Group B (control) in this test of 40 visits, users were given A or B by a simple digital flip of the coin. Given this element of randomness, it’s possible that A just happened to get more users who would’ve converted whether they got A or B. 
<div class="hero-image"><center><img class="img-responsive" src="icons/Positive_Chart.PNG" alt="Image" style="width:25%"></center></div>

<h4 class="explain">Neutral Effect</h4> 
As an example, let’s look at the following A/B results; Group A (test): 21.5 Revenue Per Customer and Group B (control): 21.5 Revenue Per Customer. The data suggests that over many visits, Group A will have 21.5 /21.5 – 1 = 0% more Revenue Per Customer. So the difference in means is 0%. In this example, the two groups have no detectable difference in performance as illustrated by the graphs.
<div class="hero-image"><center><img class="img-responsive" src="icons/Neutral_Chart.PNG" alt="Image" style="width:25%"></center></div>

<h4 class="explain">Negative Effect</h4> 
In this visual, we do not allow the control and treatment groups to toggle performance results. In other words, we constrain the visuals to only display positive or neutral effects. 


<h4 class="explain">References:</h4> 
https://en.wikipedia.org/wiki/Effect_size#Population_and_sample_effect_sizes <br>
http://www.statisticshowto.com/mean-difference/ 
 </p>
 
 </body>
</html>
